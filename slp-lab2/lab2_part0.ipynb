{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoken Language Processing - Instituto Superior Técnico\n",
    "### Laboratory Assignment 2 - Native Language Identification challenge\n",
    "\n",
    "The second laboratory assignment of the course is designed to simulate a **native language identification** challenge. In this challenge, partipants (a.k.a students enrolled in the course) receive a train, development and evaluation (blind) data set, and a set of (weak) baseline systems for the task at hand: closed-set identification of the native language of foreign English (L2) speakers in a given audio file out of a set of four target native L1 languages: Chinese,  German,  Hindi,  and  Italian.\n",
    "\n",
    "The **goal** for each participant is to develop/build the best native language identification system. To this end, participants are first required to complete the lab Notebooks and then encouraged to incorporate other techniques and explore any approach that permit improving their results.\n",
    "\n",
    "During the first week (Part 1), students are expected to:\n",
    "- Understand the main components of a simple baseline system based on **MFCC** features and **GMM** models.\n",
    "- Complete and run the main components of this baseline.\n",
    "- Propose, develop and explore simple modifications to the feature extraction process.\n",
    "- Propose, develop and explore simple modifications to the GMM native language models.\n",
    "- Evaluate the models on the development partition.\n",
    "\n",
    "During the second week (Part 2), students are expected to:\n",
    "- Understand the main components of a modern sytem based on self-supervised learning.\n",
    "- Complete and run the main components of this system.\n",
    "- Propose, develop and explore simple modifications to the native language classifiers.\n",
    "- Evaluate the models on the development partition.\n",
    "- Propose, develop, extend, combine and/or modify the different systems to obtain the best possible native language identifier.\n",
    "- Obtain predictions for the blind test partition and prepare the submission.\n",
    "\n",
    "\n",
    "<!--\n",
    "The challenge distinguishes two different tracks or evaluation conditions:\n",
    "- Track 1 - Participants are not allowed to use any kind of pre-trained model (such as x-vectors).\n",
    "- Track 2 - Participants are allowed to use anything.\n",
    "-->\n",
    "\n",
    "## About the data\n",
    "\n",
    "The data consists of mono audio files sampled at 16 kHz all of them containing English speech (L2) spoken by native speakers of one of the following L1 target languages: Chinese (`'CHI'`), German (`'GER'`), Hindi (`'HIN'`), and Italian (`'ITA'`).\n",
    "```python \n",
    "LANGUAGES = ('CHI',  'GER',  'HIN',  'ITA')\n",
    "```\n",
    "\n",
    "The dataset is organized in 4 partitions:\n",
    "- `'train'`: This is the full training set, consisting of 1200 audio samples of approximately 45-seconds each containg speech from English students with  different L1 backgrounds. (**ATENTION**: Do not use this dataset for training your models, unless your system is very fast or if you want to build your final model. It can be slow).\n",
    "- `'train100'`: This is a subset of the full training set that consists of 100 audio files per target L1 language (**RECOMMENDATION**: Use this partition in your quick experiments, to more rapidly validate alternatives)\n",
    "- `'dev'`: This is the development set. It contains the same kind of segments as the training partitions. You will typically use this to validate the quality of your model.\n",
    "- `'evl'`: This is the evaluation set. It contains the same kind of segments as the training partitions. You don't have the groud-truth for this set. You are expected to generate a prediction file and submit it.\n",
    "\n",
    "The data used in this challenge is a subset of the [ETS Corpus of Non-Native Spoken English](http://dx.doi.org/10.21437/Interspeech.2016-129).\n",
    "\n",
    "The difference is that only four L1 target languages are considered instead of eleven, and that the original development partition has been split in the development and evaluation sets used in this course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting\n",
    "The following conditions are necessary to run correctly this notebook:\n",
    "\n",
    "*   All modules included in the `requirements.txt` file need to be \n",
    "installed in the Python environment. In a local installation, you can run in the command line: `pip install -r requirements.txt`\n",
    "*   The module `pf_tools` needs to be accessible (if you are using Google Colab, you will need to copy the `pf_tools.py` every time you start a new session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pf_tools import CheckThisCell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can you download (and process) the data\n",
    "\n",
    "The first thing we have to do is to set our working directory. If you are using Google Colab, you probably want to mount Google Drive to keep persistent information, such as data, features and models. If you are not using Google Colab, you rather comment or delete the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "CheckThisCell",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCheckThisCell\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m CheckThisCell \u001b[38;5;66;03m## <---- Remove this to run this cell if you are on Google Colab\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mCheckThisCell\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise CheckThisCell ## <---- Remove this to run this cell if you are on Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is set to c:\\Users\\Piotr\\Desktop\\spoken-language-processing-23-24-IST\\slp-lab2\n",
      "Your ETS data folder is c:\\Users\\Piotr\\Desktop\\spoken-language-processing-23-24-IST\\slp-lab2/ets_data/\n"
     ]
    }
   ],
   "source": [
    "#raise CheckThisCell ## <---- Remove this after completing/checking this cell\n",
    "import os \n",
    "\n",
    "CWD = os.getcwd() # <--- Change this variable to your working directory \n",
    "DATADIR = f'{CWD}/ets_data/' # <--- Change this variable to your folder containig the ETS data\n",
    "if not os.path.isdir(DATADIR):\n",
    "    os.mkdir(DATADIR)\n",
    "\n",
    "os.chdir(CWD)\n",
    "print(f'Current working directory is set to {CWD}')   \n",
    "print(f'Your ETS data folder is {DATADIR}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `ETS` permits downloading, transforming and storing the different data partitions. Each `ETS` instance can be used to iterate over all the samples of the partition. It can also be used in combination with pytorch dataloader to read batches of data to train neural networks with pytorch. For instance, consider the following piece of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 491M/491M [00:55<00:00, 9.24MB/s] \n",
      "100%|██████████| 400/400 [00:21<00:00, 18.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pf_tools import ETS\n",
    "import librosa \n",
    "\n",
    "def audio_transform(filename):\n",
    "    y, _ = librosa.load(filename, sr=16000, mono=True)\n",
    "    return y.reshape(-1,1)\n",
    "    \n",
    "train_ets = ETS(DATADIR, 'train100', transform_id='raw', audio_transform=audio_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This will first download and uncompress the .tar.gz file containing all the necessary data of the `'train100'` partition, that is, the audio files that are stored to disk (in DATADIR/train100/audio/) and key file (DATADIR/train100/key.lst). Then, the audio transformation `'transform'` will be applied to each file and the result stored to disk DATADIR/train100/raw/. \n",
    "\n",
    "(If the feature extraction process is interrupted, you will need to delete the corresponding tranformation folder to restart)\n",
    "\n",
    "**Audio transformations** receive a filename of an audio file and returns an array of dimensions (NxD), in which N is the time dimension and D the dimension of the feature vector. In this simple case D is 1 because the transform is just returning the raw audio signal.\n",
    "\n",
    "The `ETS` class permits chunking the output of the audio transformation (of size NxD) in chunks of CxD size. The chunking operation divides the result of the transformation process, in multiple smaller pieces with a configurable chunk size and hop length. These chunks can be further transformed and stored as individual feature files. The number of chunks depends on the size of the original file and the hop length. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:07<00:00, 51.36it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ets = ETS(DATADIR, 'train100', \n",
    "                     transform_id='chunks', \n",
    "                     audio_transform=audio_transform, \n",
    "                     chunk_size=10*16000, \n",
    "                     chunk_hop=5*16000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will download and uncompress the partition data, only if was not already done before. Then, as previously, the simple tranform that returns the waveform is applied to each audio file. After this, the resulting array of dimension Nx1, in which N=16000xduration_in_seconds, is split in continuous chunks of length 160000 (that is, 10 seconds) with chunk hop of 5 seconds. Each one of these chunks of 10 seconds is stored and will be accessed whenever we iterate the dataset. \n",
    "\n",
    "Aditionally, the optional argument `chunk_transform` permits defining a transformation to be applied to each chunk before storing to disk. It can be any function that receives an array of size CxD and returns an array HxW, in which H is the *new time dimension*. For instance, the following example takes the audio segments of 160000x1, computes the mean and variance every 0.1 sec (1600 samples) and returns a feature vector of size 100x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:07<00:00, 56.68it/s]\n"
     ]
    }
   ],
   "source": [
    "def chunk_transform(x):\n",
    "    x = x.reshape(-1,1600)\n",
    "    return np.concatenate((x.mean(axis=1, keepdims=True), x.std(axis=1, keepdims=True)),axis=1)\n",
    "\n",
    "train_ets = ETS(DATADIR, 'train100', \n",
    "                     transform_id='chunks_mv', \n",
    "                     audio_transform=audio_transform, \n",
    "                     chunk_size=10*16000, \n",
    "                     chunk_hop=5*16000, \n",
    "                     chunk_transform=chunk_transform)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice that, while the above example is probably useless as an effective feature extraction method, the proper combination of audio and chunk transformations is expected to permit quite flexible feature extraction that (hopefully) can match the needs of almost any training setting. \n",
    "\n",
    "Once we have instanciated a ETS dataset, it can be iterated to have access to each processed sample, for instance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (100, 2) HIN train_0005\n",
      "300 (100, 2) CHI train_0110\n",
      "600 (100, 2) ITA train_0224\n",
      "900 (100, 2) CHI train_0332\n",
      "1200 (100, 2) HIN train_0434\n",
      "1500 (100, 2) ITA train_0556\n",
      "1800 (100, 2) CHI train_0652\n",
      "2100 (100, 2) GER train_0742\n",
      "2400 (100, 2) GER train_0849\n",
      "2700 (100, 2) CHI train_0960\n",
      "3000 (100, 2) GER train_1057\n",
      "Finished reading all data in 16.211992979049683\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for i, sample in enumerate(train_ets):\n",
    "    data, label, basename = sample # array, str, str\n",
    "    if i % 300 == 0:\n",
    "        print(i, data.shape, label, basename)\n",
    "\n",
    "print(f'Finished reading all data in {time.time() - start}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the `ETS` class to check the  number of files and size (in minutes) of the training set for each target language. You should keep these numbers to include in your system description paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHI:\t100\t77.44 minutes\n",
      "GER:\t100\t77.58666666666667 minutes\n",
      "HIN:\t100\t77.58666666666667 minutes\n",
      "ITA:\t100\t77.59333333333333 minutes\n"
     ]
    }
   ],
   "source": [
    "# Inspect the training data to find the size of each training language\n",
    "#raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "LANGUAGES = ('CHI',  'GER',  'HIN',  'ITA')    \n",
    "train_ets = ETS(DATADIR, 'train', transform_id='raw', audio_transform=audio_transform)\n",
    "train_ets = ETS(DATADIR, 'train100', transform_id='raw', audio_transform=audio_transform)\n",
    "\n",
    "num_files = dict().fromkeys(LANGUAGES, 0)\n",
    "num_samples = dict().fromkeys(LANGUAGES, 0)\n",
    "\n",
    "# <----- LAB WORK: ADD YOUR CODE HERE\n",
    "for item in train_ets:\n",
    "    data, label, basename = item\n",
    "    num_files[label] += 1\n",
    "    num_samples[label] += data.shape[0]\n",
    "\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    print(f'{lang}:\\t{num_files[lang]}\\t{num_samples[lang]/(60*16000)} minutes')\n",
    "\n",
    "# The expected output should be something like for the train100:\n",
    "# CHI:\t100 files\t77.44 minutes\n",
    "# GER:\t100 files\t77.58666666666667 minutes\n",
    "# HIN:\t100 files\t77.58666666666667 minutes\n",
    "# ITA:\t100 files\t77.59333333333333 minutes\n",
    "#\n",
    "# And the following gor train:\n",
    "# CHI:\t300\t232.38666666666666 minutes\n",
    "# GER:\t300\t232.79333333333332 minutes\n",
    "# HIN:\t300\t232.63333333333333 minutes\n",
    "# ITA:\t300\t232.70666666666668 minutes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `ETS` class extends the `torch.utils.data.Dataset` and it can be used in combination with a Pytorch DataLoader to read data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 2]) 10 10\n",
      "torch.Size([10, 100, 2]) 10 10\n",
      "torch.Size([10, 100, 2]) 10 10\n",
      "torch.Size([10, 100, 2]) 10 10\n",
      "Finished reading all data in 13.179200172424316\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "train_ets= ETS(DATADIR, 'train100', \n",
    "                     transform_id='chunks_mv', \n",
    "                     audio_transform=audio_transform, \n",
    "                     chunk_size=10*16000, \n",
    "                     chunk_hop=5*16000, \n",
    "                     chunk_transform=chunk_transform)\n",
    "\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "        dataset=train_ets,\n",
    "        batch_size=10,\n",
    "        shuffle=True\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "for i, batch in enumerate(train_batches):\n",
    "    data, label, basename = batch\n",
    "    if i % 100 == 0:\n",
    "        print(data.shape, len(label), len(basename))\n",
    "\n",
    "print(f'Finished reading all data in {time.time() - start}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting Part 1 of this lab assignment, you should delete the folders containing the dummy features that you just generated: _'raw'_, _'chunks'_, _'chunks_mv'_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What should you deliver at the end of this lab assignment?\n",
    "You should deliver the following three elements:\n",
    "- You must submit (via [Kaggle](https://www.kaggle.com/t/312cd4200cfb4e138ea9372ce5bc33fd) and Fênix) at least one prediction file in the format that will be described in the Notebook of part1.\n",
    "- You must submit (via Fênix) all the modified notebooks and any additional code used for your proposed system(s).\n",
    "- You must submit a report (via Fênix) of maximum 2 pages describing your work, your system(s), approaches explored (may be unsuccesful), parameters explored, lessons learnt, results on the dev partition, etc. You can use the following Overleaf template for the report: [report](https://www.overleaf.com/latex/templates/interspeech-2023-paper-kit/kzcdqdmkqvbr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contacts and support\n",
    "You can contact the professors during the classes or the office hours.\n",
    "\n",
    "Particularly, for this second laboratory assignment, you should contact Prof. Alberto Abad: alberto.abad@tecnico.ulisboa.pt\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
