{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # <span style=\"color:red\">UNDER CONSTRUCTION!!!!</span> --> \n",
    "\n",
    "# Spoken Language Processing - Instituto Superior Técnico\n",
    "## Laboratory Assignment 2 - Native Language Identification challenge\n",
    "\n",
    "# PART 2 - Fine-tuning a self-supervised pre-trained model\n",
    "\n",
    "\n",
    "This notebook contains the guide and code cells that permit implementing an advanced system for native language identification based on fine-tuning of a self-supervised pre-trained model. Besides, the notebook will show how to obtain predictions and score the systems on the development set.\n",
    "\n",
    "**In contrast to previous notebook, we will make use of some scripts that are part of the S3PRL framework. These are typically run in a terminal, so, some of the following steps may be simpler to run in a termnial, rather than as cell in the Notebook itself.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting\n",
    "\n",
    "Let's import some modules and make some definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import csv \n",
    "\n",
    "from pf_tools import CheckThisCell\n",
    "\n",
    "LANGUAGES = ('CHI',  'GER',  'HIN',  'ITA')\n",
    "LANG2ID = {'CHI':1, 'GER':2, 'HIN':3, 'ITA':4}\n",
    "ID2LANG = dict((LANG2ID[k],k)for k in LANG2ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the previous Notebooks, you need to mount Google drive if you are working on Google Colab. Otherwise, you should skip or delete the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raise CheckThisCell ## <---- Remove this torun this cell if you are on Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Like in Part1, the audio data is expected to be located in a folder with the following format:\n",
    "\n",
    "```\n",
    "ets_data/\n",
    "├── train/\n",
    "│   └── audio/\n",
    "│       └──wav files\n",
    "│   └── key.lst \n",
    "│\n",
    "└── train100/\n",
    "    └── audio/\n",
    "        └──wav files\n",
    "    └── key.lst\n",
    "... \n",
    "```\n",
    "\n",
    "You must already have this from part 1, so you can set-up your data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "CWD = os.getcwd()\n",
    "DATADIR = f'{CWD}/ets_data/' # <--- Change this variable to your working directory containig the ETS data\n",
    "if not os.path.isdir(DATADIR):\n",
    "    os.mkdir(DATADIR)\n",
    "    print(f'WARNING: Your data is not in the folder {DATADIR}')\n",
    "\n",
    "os.chdir(CWD)\n",
    "print(f'Your ETS data should be in this folder {DATADIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to download again the data, you can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise CheckThisCell\n",
    "\n",
    "os.chdir(DATADIR)\n",
    "\n",
    "# download train\n",
    "!wget http://groups.tecnico.ulisboa.pt/speechproc/pf24/lab2/train.tgz\n",
    "!tar -xzvf train.tgz\n",
    "\n",
    "#download train100\n",
    "!wget http://groups.tecnico.ulisboa.pt/speechproc/pf24/lab2/train100.tgz\n",
    "!tar -xzvf train100.tgz\n",
    "\n",
    "#download dev\n",
    "!wget http://groups.tecnico.ulisboa.pt/speechproc/pf24/lab2/dev.tgz\n",
    "!tar -xzvf dev.tgz\n",
    "\n",
    "#download evl\n",
    "!wget http://groups.tecnico.ulisboa.pt/speechproc/pf24/lab2/evl.tgz\n",
    "!tar -xzvf evl.tgz\n",
    "\n",
    "os.chdir(CWD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The SSL based model\n",
    "\n",
    "The goal of this part of the laboratory is to expose students to modern tools and methods for speech classification.\n",
    "In particular, we will use the [s3prl](https://github.com/s3prl/s3prl) toolkit to build a native language identification system based on self-supervised learning (SSL) models as feature extraction.\n",
    "\n",
    "[s3prl](https://github.com/s3prl/s3prl) is an open source toolkit, which stands for Self-Supervised Speech Pre-training and Representation Learning. Self-supervised speech pre-trained models are called upstream in this toolkit, and are utilized in various downstream tasks.\n",
    "\n",
    "The toolkit permits pre-training upstream models, load already pre-trained upstream models and/or utilize these upstream models in lots of downstream tasks already defined.\n",
    "\n",
    "For this lab, the faculty team configured a downstream task and a simple model specifically for our native language identification task and data. The model consists of a projection layer, followed by an average pooling, and a linear output layer.\n",
    "\n",
    "In this part of the lab, students are  expected to *play* with the different upstream models to build the best possible native language identification system. \n",
    "In particular, students are encouraged to explore and discover which of the available SSL models can be a better candidate for their classification system. Note that using a large SSL model will turn out the training process very slow. So, you should choose wisely depending for instance on the reported performance in similar tasks.\n",
    "\n",
    "Besides playing with the different upstream models, interested students can try to modify some of the details of the \"expert\" downstream model. This can be done relatively easy using one of the many examples already included in the toolkit as a starting point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Installing the toolkit\n",
    "Let's start by cloning the repository of the s3prl toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/s3prl/s3prl.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, a new folder named  `s3prl/` with the contents of the toolkit has been created. We'll now install the toolkit itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3PRLDIR = CWD + '/s3prl/'\n",
    "os.chdir(S3PRLDIR)\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Configuring the downstream task\n",
    "Let's create the downstream native language identification task. In the `s3prl/downstream/` there are plenty of examples. The faculty team took one of those as an example to create the configuration needed for this lab assignment. Let's download and copy it to the downstream folder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(f'{S3PRLDIR}/s3prl/downstream/') # <----- change to the downstream folder\n",
    "!wget http://groups.tecnico.ulisboa.pt/speechproc/pf24/lab2/nli_s3prl_downstream.tgz # <--- download the lab specific downstream task\n",
    "!tar -xzvf nli_s3prl_downstream.tgz  # <---- unzip\n",
    "!rm nli_s3prl_downstream.tgz\n",
    "os.chdir(S3PRLDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look to the contents of the folder `s3prl/downstream/native_language_identification/`. There are some important files that help to define the task:\n",
    "- `dataset.py`: this file provides the class that permits loading the ETS data. Something similar to the ETS class used in part1, but following the formatting rules of the s3prl toolkit. **You don't need to change anything here**.\n",
    "- `expert.py`: this file defines the expert downstream task. In this case, the expert takes the ouput of the upstream model (configurable), applies a projection layer, and then a classification model (configurable) to obtain the final predictions. **You don't need to change anything here**.\n",
    "- `model.py`: this file contains the definitions of the model after the projection. We could include several configurations that can later be selected when we run the actual experiment. The model included is just an average pooling (that reduces the time dimension to a single vector) followed by a linear output layer. **You don't need to change anything here, but you may want to explore other configurations following the examples of other downstream tasks included in s3prl**.\n",
    "- `config.yaml`: this file permits configuring some parameters of your experiment, including the path that contains the task data and the training set that is going to be used (either train or train100). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure our experiment: \n",
    "\n",
    "Edit the 'file_path' entry in the configuration file `downstream/native_language_identification/config.yaml` to the folder containing the data:\n",
    "\n",
    "```yaml\n",
    "downstream_expert:\n",
    "    datarc:\n",
    "        file_path: \"your_path/ets_data\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "And also edit the config file, to use either the \"train100\" partition or the total training data \"train\"  by just editing the following entry of the `downstream/native_language_identification/config.yaml`:\n",
    "\n",
    "```yaml\n",
    "downstream_expert:\n",
    "    datarc:\n",
    "        ...\n",
    "        train: \"train100\"\n",
    "```\n",
    "\n",
    "You may also want to reduce the number of training steps to 1000 or 2000 for quick experimentation of different configurations:\n",
    "\n",
    "```yaml\n",
    "runner:\n",
    "  total_steps: 5000\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.4 Training the downstream model and classification of the dev set\n",
    "Now it's time for training. For that, we will use the Pythons script `run_downstream.py` in train mode and in which we will set:\n",
    "- an arbitrary name for identifying this experiment, in which the results will be saved ( `ExpName`)\n",
    "- the upstream model to be used, for instance `fbank`. You can check for more SSL pretrained available models  here https://s3prl.github.io/s3prl/tutorial/upstream_collection.html\n",
    "- the downstream task, in this case \"native_language_identification\"\n",
    "\n",
    "\n",
    "```bash\n",
    "python3 run_downstream.py -n ExpName -m train -u fbank -d native_language_identification\n",
    "```\n",
    "\n",
    "Since this can take a while (actually, a lot depending on the chosen upstream model), you probably want to run this in a terminal, rather than inside the Notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(f'{S3PRLDIR}/s3prl')\n",
    "!python3 run_downstream.py -n fbank -m train -u fbank -d native_language_identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training step generated a folder containing training results in `result/downstream/{ExpName}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(f'{S3PRLDIR}/s3prl')\n",
    "!ls result/downstream/fbank/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting files: the `dev_predict.txt` contains the predictions on the dev set, the `dev-best.ckpt` contains the model parameters for the best checkpoint and the log.log contains information of the training process, including the identification accuracy in the train and dev sets. Notice that we will later use the `dev_predict.txt`  file together with the predictions of the `evl` set to create the submission file to upload to Kaggle.\n",
    "\n",
    "If the training process is interrupted, you can continue from the last saved checkpoint (a checkpoint is saved every 200 iterations):\n",
    "\n",
    "```bash\n",
    "python3 run_downstream.py -m train -e result/downstream/fbank/states-2000.ckpt\n",
    "```\n",
    "\n",
    "Finally, note that the `fbank` is a very bad upstream model. You need to try other upstream models, you can start with those commented in the theoretical lessons or the ones that have shown good performance in similar tasks. Be careful (and wise) in your decisions: experiments may be slow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.5 Classification of the evl set \n",
    "\n",
    "Now that we already trained our downstream model, we can use it to predic on the blind evl set. For this purpose, we will use again the `run_downstream.py` script in evaluate mode and we need to select the actual model to use:\n",
    "\n",
    "\n",
    "```bash\n",
    "python3 run_downstream.py -m evaluate -e result/downstream/fbank/dev-best.ckpt\n",
    "```\n",
    "\n",
    "NOTE: Ignore the test accuracy reported at the end (we actually don't have the groundtruth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 run_downstream.py -m evaluate -e result/downstream/fbank/dev-best.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `test_predict.txt` file contains the predictions of this model for the evl set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Create the final predictions file and submit to the challenge\n",
    "\n",
    "Like in Part 1, we will create the predictions file in the expected format.\n",
    "The predictions file used for submission and scoring is a CSV file containing the predictions of both the `dev` and `evl` partitions.\n",
    "The file has two fields: fileId and Lang. The fileId is the unique audio file identifier and the Lang field is the language prediction (numeric from 1 to 4). The predictions file name must be as follows:\n",
    "\n",
    "`G<YY>_<SYSTEMID>.csv` \n",
    "\n",
    "where `<YY>` is the students' group number (use 2 digits) and `<SYSTEMID>` is an identifying string for that submission/system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainset = 'train100'\n",
    "upstream_id = 'fbank' ## <--- CHANGE THIS ACCORDINGLY\n",
    "group, system = '00', f'ssl_{trainset}_{upstream_id}'\n",
    "\n",
    "filename_dev = f'{S3PRLDIR}/s3prl/result/downstream/{upstream_id}/dev_predict.txt'\n",
    "filename_evl = f'{S3PRLDIR}/s3prl/result/downstream/{upstream_id}/test_predict.txt'\n",
    "\n",
    "\n",
    "with open(f'{CWD}/g{group}_{system}.csv', 'w') as file:\n",
    "    \n",
    "    csv_writer = csv.writer(file) # CSV writer\n",
    "    csv_writer.writerow(('fileId', 'Lang')) # Header of the CSV\n",
    "\n",
    "    results_dev = [l.strip().split() for l in open(filename_dev, 'r')]\n",
    "    results_evl = [l.strip().split() for l in open(filename_evl, 'r')]\n",
    "\n",
    "    # Save dev results\n",
    "    for file_id, lang in results_dev:\n",
    "        file_id = file_id.split('-')[-1]\n",
    "        lang = LANG2ID[lang]\n",
    "        csv_writer.writerow((file_id, lang))\n",
    "        \n",
    "    # Save evl results\n",
    "    for file_id, lang in results_evl:\n",
    "        file_id = file_id.split('-')[-1]\n",
    "        lang = LANG2ID[lang]\n",
    "        csv_writer.writerow((file_id, lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can submit your prediction(s) in the following [Kaggle competition](https://www.kaggle.com/t/312cd4200cfb4e138ea9372ce5bc33fd).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contacts and support\n",
    "You can contact the professors during the classes or the office hours.\n",
    "\n",
    "Particularly, for this second laboratory assignment, you should contact Prof. Alberto Abad: alberto.abad@tecnico.ulisboa.pt\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
