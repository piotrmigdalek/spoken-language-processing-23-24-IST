{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoken Language Processing - Instituto Superior TÃ©cnico\n",
    "## Laboratory Assignment 2 - Native Language Identification challenge\n",
    "\n",
    "# PART 1 - Classical models based on conventional features\n",
    "\n",
    "\n",
    "This notebook contains the guide and code cells (some of them partially incomplete) that permit implementing a baseline system for native language identification based on conventional feeatures and a simple generative model. Besides, the notebook will show how to obtain predictions and  score the systems on the development set. \n",
    "\n",
    "**Read carefully the Markdown information, but also the comments inside the code cells (they provide useful information and hints), and also the code itself. The better you understand it, the easier will be modyfing it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting\n",
    "\n",
    "Like in the introduction Notebook, we'll import the dummy Exception class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pf_tools import CheckThisCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And  you need to mount Google drive if you are working on Google Colab. If you are note using Google Colab, skip or delete the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "CheckThisCell",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCheckThisCell\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m CheckThisCell \u001b[38;5;66;03m## <---- Remove this torun this cell if you are on Google Colab\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mCheckThisCell\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise CheckThisCell ## <---- Remove this torun this cell if you are on Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set-up your working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "import os \n",
    "\n",
    "CWD = os.getcwd() # <--- Change this variable to your working directory \n",
    "DATADIR = f'{CWD}/ets_data/' # <--- Change this variable to your folder containig the ETS data\n",
    "if not os.path.isdir(DATADIR):\n",
    "    os.mkdir(DATADIR)\n",
    "\n",
    "os.chdir(CWD)\n",
    "print(f'Current working directory is set to {CWD}')   \n",
    "print(f'Your ETS data folder is {DATADIR}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The MFCC and GMM baseline system \n",
    "This baseline consists of MFCC feature extraction  (based on the `librosa` module) with the following (optional) additional commponents:\n",
    "- Delta and Double-delta computation;\n",
    "- Shifted Delta Cepstrum (SDC), an alternative to deltas used in language classification;\n",
    "- Voice Activity Detection (VAD);\n",
    "- Cepstral mean and variance normalization (CMVN).\n",
    "\n",
    "The feature extraction module is followed by GMMs of 64 dimensions for each language (using the `sklearn` module). \n",
    "\n",
    "Student groups will be graded depending on their ability to develop the different modules, propose alternatives, and evaluate and compare different configurations (delta vs SDC, GMM dimension, using or not using VAD or CMVN, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Initialization and importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pf_tools import ETS\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import csv \n",
    "from tqdm import tqdm\n",
    "\n",
    "GLOBAL_SEED = 35731\n",
    "\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "\n",
    "\n",
    "LANGUAGES = ('CHI',  'GER',  'HIN',  'ITA')\n",
    "LANG2ID = {'CHI':1, 'GER':2, 'HIN':3, 'ITA':4}\n",
    "ID2LANG = dict((LANG2ID[k],k)for k in LANG2ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The MFCC feature extraction module\n",
    "The next function extracts MFCCs, but there are plenty of things that can be improved. You are free to change anything you want, including the number of formal parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read carefully this function and understand it\n",
    "#raise CheckThisCell ## <---- Remove this after completing/checking this cell\n",
    "\n",
    "def feat_extract(filename, orig_sr=16000, mono=True, n_mfcc = 13, remove_c0=False, delta_order=0, apply_sdc=False, apply_vad=False, apply_cmvn=False, skip_frames=2):\n",
    "    \n",
    "    sr=16000\n",
    "    n_mels = 40\n",
    "    n_fft = 512 \n",
    "    hop_length = 160\n",
    "    fmin = 50\n",
    "    fmax = 7800\n",
    "    \n",
    "    # Both delta and sdc are not possible\n",
    "    if apply_sdc and (delta_order > 0):\n",
    "        raise ValueError(\"Applying SDC and delta > 0 is not compatible\")\n",
    "\n",
    "\n",
    "    # Load audio wav into numpy array\n",
    "    y, _ = librosa.load(filename, sr=orig_sr, mono=mono)\n",
    "    \n",
    "    # Resample in case it's needed\n",
    "    if orig_sr != sr:\n",
    "        y = librosa.resample(y, orig_sr=orig_sr,target_sr=sr)\n",
    "\n",
    "    ## OPTIONAL ADDIDITIONAL STAGES \n",
    "    # 0 - PREPROCESSING - Typical preprocessing may include normalization of audio (mean removal), \n",
    "    #                       but also speech enhancement and others more complex. \n",
    "    #                       You can try this at a later stage, this is 100% optional\n",
    "\n",
    "    # Extract MFFCs\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_fft=n_fft, \n",
    "                                n_mfcc=n_mfcc, n_mels=n_mels, \n",
    "                                hop_length=hop_length, \n",
    "                                fmin=fmin, fmax=fmax, htk=False).T\n",
    "    \n",
    "    \n",
    "    ## ADDIDITIONAL STAGES - LAB WORK\n",
    "\n",
    "    # 1 Compute deltas --> Hint you can use librosa (see below)\n",
    "    if delta_order > 0:\n",
    "        mfcc = compute_delta(mfcc, delta_order=delta_order)\n",
    "    \n",
    "    # 2 SDC --> Hint: You can build this using deltas (of extented context)  \n",
    "    if apply_sdc:\n",
    "        mfcc = compute_sdc(mfcc)\n",
    "    \n",
    "    # 3 COMPUTE VAD --> Hint: You can use any vad (theshold on rms energy, something avaialble in the net, ...).\n",
    "    #                         Coeff0 of MFCC is highly related with Energy and it can be used as a proxy\n",
    "    #                   ATTENTION: Using a VAD may have a significant impact  \n",
    "    if apply_vad:\n",
    "        mfcc, _ = compute_vad(mfcc)\n",
    "             \n",
    "    # 4 APPLY CMVN --> ATTENTION: Using normalization may have a significant impact\n",
    "    if apply_cmvn:\n",
    "        mfcc = compute_cmvn(mfcc)\n",
    "        \n",
    "    if skip_frames > 0: # remove begining and ending frames, which are tupically unreliable\n",
    "        mfcc = mfcc[skip_frames:-skip_frames]\n",
    "\n",
    "    return mfcc, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the additional modules, let's extract MFCCs of one audio file using `feat_extract`. Notice that this function returns two arguments: an array containing the features and the audio waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATADIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mDATADIR\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train100/audio/train_0005.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m mfcc, audio \u001b[38;5;241m=\u001b[39m feat_extract(audio_file, apply_cmvn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, apply_sdc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(mfcc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATADIR' is not defined"
     ]
    }
   ],
   "source": [
    "audio_file = f'{DATADIR}/train100/audio/train_0005.wav'\n",
    "mfcc, audio = feat_extract(audio_file, apply_cmvn=False, apply_sdc=False)\n",
    "print(mfcc)\n",
    "# We can plot the audio waveform and the features\n",
    "fig, ax = plt.subplots(nrows=2, sharex=False)\n",
    "ax[0].plot(audio)\n",
    "ax[0].set(title='Audio waveform')\n",
    "\n",
    "# notice the transposition before calling the display: librosa expects arrays of DxT, in which T is time, \n",
    "# however our feature extraction returns arrays of TxD\n",
    "img = librosa.display.specshow(mfcc.T,  ax=ax[1])\n",
    "# fig.colorbar(img, ax=[ax[1]])\n",
    "ax[1].set(title='MFCC')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the output arrays and confirm they have the expected dimension (use `audio.shape` and `mfcc.shape`). What is the relation among the dimensions of audio and mfcc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise CheckThisCell ## <---- Remove this after completing/checking this cell\n",
    "audio.shape, mfcc.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to run the data processing stage for the train100 partition we will simply instanciate the ETS class as described in the introduction Notebook. Take a sit because it can take a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATADIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain100\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m transform_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmfcc13\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 16\u001b[0m train_ets \u001b[38;5;241m=\u001b[39m ETS(\u001b[43mDATADIR\u001b[49m, trainset, \n\u001b[1;32m     17\u001b[0m                  transform_id\u001b[38;5;241m=\u001b[39mtransform_id, \n\u001b[1;32m     18\u001b[0m                  audio_transform\u001b[38;5;241m=\u001b[39mtransform[transform_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_transform\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     19\u001b[0m                  chunk_transform\u001b[38;5;241m=\u001b[39mtransform[transform_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_transform\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     20\u001b[0m                  chunk_size\u001b[38;5;241m=\u001b[39mtransform[transform_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     21\u001b[0m                  chunk_hop\u001b[38;5;241m=\u001b[39mtransform[transform_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_hop\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m                 )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATADIR' is not defined"
     ]
    }
   ],
   "source": [
    "# We will use a dictionary to store the different feature extraction configurations\n",
    "transform = {'mfcc13' : \n",
    "                 { \n",
    "                     'audio_transform': lambda x : feat_extract(x, orig_sr=16000, mono=True, n_mfcc = 13, apply_sdc=False, apply_vad=False, apply_cmvn=False)[0],\n",
    "                     'chunk_transform': None, \n",
    "                     'chunk_size': -1, # <--- a negative value mean the whole file: we dont chunk the audio and\n",
    "                                       #      we compute a feature matrix for each complete audio file\n",
    "                     'chunk_hop': -1\n",
    "                 }\n",
    "            }\n",
    "\n",
    "\n",
    "trainset = 'train100'\n",
    "transform_id = 'mfcc13'\n",
    "\n",
    "train_ets = ETS(DATADIR, trainset, \n",
    "                 transform_id=transform_id, \n",
    "                 audio_transform=transform[transform_id]['audio_transform'], \n",
    "                 chunk_transform=transform[transform_id]['chunk_transform'],\n",
    "                 chunk_size=transform[transform_id]['chunk_size'], \n",
    "                 chunk_hop=transform[transform_id]['chunk_hop']\n",
    "                )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your current folder, many things happened!! \n",
    "\n",
    "Notice that if you instanciate again the ETS class for the 'train100' partition, the data will not be downloaded again. \n",
    "Additionally, if there is already a folder with the name `transform_id`, feature extraction will not run again. You need to delete from your filesystem the folder with the features if you want to run again the feature extraction (using the same identifier) or , alternatively, you can change the identifier. Be careful because you can easily increase the amount of data generated. If you try a feature extraction method that provides bad results, you probably don't want to keep the features in disk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the basics, it is time to try to improve your features. To do this, try to define some or all of the following steps to improve your feature extraction pipeline (students will be evaluated depending on their ability to solve the following functions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy arrays have methods to compute mean and variance, so this one should be really easy.\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.std.html\n",
    "# \n",
    "# Be careful about the dimensions!! You want to compute mean and variance over the time dimension!!\n",
    "\n",
    "#raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "def compute_cmvn(features):\n",
    "\n",
    "    mean_features=np.mean(features, axis=)\n",
    "    var_features=np.std(features, axis=)\n",
    "    \n",
    "    return features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librosa contains functions to compute deltas: https://librosa.org/doc/main/generated/librosa.feature.delta.html\n",
    "# Ideally, this function should permit choosing  the size of the window to compute the deltas, \n",
    "# the maximum order (order=1 is delta, order=2 is delta-delta and so on). \n",
    "# When selecting order > 1 , it is exepected that ALL the delta components are \n",
    "# appended to the feature vector. For instance, if the features are of dimension D, \n",
    "# selecting order 2 will contactenate both the velocity and the acceleration.\n",
    "# Additionally the functions must permit keeping the static MFCCs. \n",
    "# It is FOUNDAMENTAL TO KEEP THEM, DYNAMIC ALONE WILL NOT WORK WELL!!\n",
    "# \n",
    "# BE CAREFUL IF YOU USE LISBROSA, it expects the time dimension to be the last one, in other words, or you  transpose\n",
    "# and transpose back, or you select the proper axis!! \n",
    " \n",
    "#raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "def compute_delta(features, win=3, delta_order=2, keep_static=True):\n",
    "    if keep_static==False:\n",
    "        \n",
    "        if delta_order<1:\n",
    "            print(\"Third parameter should be greater than 1\")\n",
    "        else:\n",
    "            for i in range(delta_order):\n",
    "                delta=[]\n",
    "                delta = librosa.feature.delta(features, width=win, order=i, axis=0)\n",
    "                features = np.concatenate((features, delta), axis=0)\n",
    "            \n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDC features were a quite popular method to increase context in classic language recognition systems. \n",
    "# An illustration can be found here: \n",
    "# https://www.researchgate.net/profile/Ravi-Vuddagiri/publication/328067371/figure/fig1/AS:677988265893889@1538656407455/Computing-Shifted-Delta-Cepstra-SDC-feature-vector-at-frame-t-for-parameters-N-d-P.ppm\n",
    "# Compute deltas (with extra/repeated vectors at begining and end) and then select previous and next deltas with fixed intervals to cocatentate. \n",
    "# You may need to code a bit here or find some function that helps with this\n",
    "raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "def compute_sdc(features, P=3, D=1, K=7, keep_static=True):\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can think of several strattegies to compute VAD, simple ones based on energy and a threshold, or maybe some more \n",
    "# ellaborated ones, like training a GMM with 2 mixtures with the Energy. \n",
    "# In addition to the features without the silence frames, \n",
    "# this function may return a sequence of booleans to help you to validate the method.\n",
    "# You can use librosa to obtain the energy of each frame (use same framing config as for the MFCCs)\n",
    "# https://librosa.org/doc/main/generated/librosa.feature.rms.html\n",
    "# Or alternatively, \n",
    "# the first mfcc coefficient, which is a good proxy for the energy\n",
    "\n",
    "raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "def compute_vad(features, energy=None, y=None):\n",
    "    vad = np.isreal(features[:,0])\n",
    "    return features[vad], vad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second returned expression of the VAD function is expected to be a boolean vector\n",
    "# of one True or False, one per frame of the feature matrix. You can see the output of it and verify \n",
    "# if it's doing what is expected (removing low energy features) \n",
    "raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "mfcc_wo_vad, vad = compute_vad(mfcc,y=audio)\n",
    "plt.plot(audio[:5*16000])\n",
    "plt.plot(reduce(lambda a,b:a+b, ((0.3,)*160 if s else (0,)*160 for s in vad[:500])), 'r')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep testing using an audio file with different configurations of the feature extraction Inspect the dimensions, verify that your code is doing what is expected, inspect and visualize the data using the previous examples and some of the lessons learnt in LAB1. You can also listen to some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "audio_file = f'{DATADIR}/train100/audio/train_0005.wav'\n",
    "mfcc_sdc, _ = feat_extract(audio_file, apply_cmvn=False, apply_sdc=True)\n",
    "mfcc_dd, _ = feat_extract(audio_file, apply_cmvn=False, delta_order=2)\n",
    "mfcc_d, _ = feat_extract(audio_file, apply_cmvn=False, delta_order=1)\n",
    "mfcc, _ = feat_extract(audio_file, apply_cmvn=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done completing the additional modules, you can rerun the feature extraction process for the training set using new configurations. You can add new entries to the dictionary keeping several transformation configurations and instantiate the ETS class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "\n",
    "## <--- YOU CAN ADD NEW CONFIGURATIONS\n",
    "transform ['mfcc39_d_dd_vad_cmvn'] =  { \n",
    "                     'audio_transform': lambda x : feat_extract(x, orig_sr=16000, mono=True, n_mfcc = 13, delta_order=2, apply_sdc=False, apply_vad=True, apply_cmvn=True)[0],\n",
    "                     'chunk_transform': None, \n",
    "                     'chunk_size': -1, # <--- a negative value means the whole file: we dont chunk the audio and\n",
    "                                       #      we compute a feature vector for each complete audio file\n",
    "                     'chunk_hop': -1\n",
    "                 }\n",
    "            \n",
    "transform['mfcc_sdc_vad_cmvn'] = { \n",
    "                     'audio_transform': lambda x : feat_extract(x, orig_sr=16000, mono=True, n_mfcc = 7, apply_sdc=True, apply_vad=True, apply_cmvn=True)[0],\n",
    "                     'chunk_transform': None,\n",
    "                     'chunk_size': -1, \n",
    "                     'chunk_hop':-1\n",
    "                }\n",
    "              \n",
    "trainset = 'train100'\n",
    "# transform_id = 'mfcc39_d_dd_vad_cmvn' # <--- select the configuration you want to use\n",
    "# transform_id = 'mfcc_sdc_vad_cmvn'\n",
    "transform_id = 'mfcc13'\n",
    "\n",
    "\n",
    "train_ets = ETS(DATADIR, trainset, \n",
    "                 transform_id=transform_id, \n",
    "                 audio_transform=transform[transform_id]['audio_transform'], \n",
    "                 chunk_transform=transform[transform_id]['chunk_transform'],\n",
    "                 chunk_size=transform[transform_id]['chunk_size'], \n",
    "                 chunk_hop=transform[transform_id]['chunk_hop']\n",
    "                )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 GMMs for L1 modeling\n",
    "The model in this baseline is extremely simple: we'll train an individual GMM model for each L1 language on top of the features that we just extracted. Later, in prediction time, given a test audio sample, we'll compute the loglikelihood obtained with each GMM model and select as the identified L1 native language the one whose model gives the highest likelihood. Let's go for it!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN GMM training each training sample contains more than one frame;\n",
    "# so we cocatenate all data to have all training data in one array \n",
    "# and the corresponding label with same time duration\n",
    "\n",
    "\n",
    "raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "\n",
    "# We will train using the following configuration\n",
    "transform_id = 'mfcc13' # <--- CHANGE THIS\n",
    "trainset = 'train100'   # <--- CHANGE THIS\n",
    "train_ets = ETS(DATADIR, trainset, \n",
    "                 transform_id=transform_id, \n",
    "                 audio_transform=transform[transform_id]['audio_transform'], \n",
    "                 chunk_transform=transform[transform_id]['chunk_transform'],\n",
    "                 chunk_size=transform[transform_id]['chunk_size'], \n",
    "                 chunk_hop=transform[transform_id]['chunk_hop']\n",
    "                )\n",
    "\n",
    "start = time.time()\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for data, label, basename in train_ets:\n",
    "        train_data.append(data)\n",
    "        train_labels.append(np.full(data.shape[0], label)) \n",
    "\n",
    "train_data = np.concatenate(train_data)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "\n",
    "print(f'Finished reading all data in {time.time() - start}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two arrays containing the complete training dataset and the corresponging reference labels. Check the sizes. You can have a look to the content of one time instant. Do some checks on the data to be sure that everything is as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the traininig data. Notice that if you apply VAD, the size of the training data must be smaller than the complete data set. \n",
    "# Register the size in frames and in time of training data for each language\n",
    "\n",
    "raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "train_data.shape, train_labels[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go training. Again, depending of the amount of data used, the model complexity and computational resources of the machine that you're using, this can take a while. So, relax while the computer works for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN GMM models (ML) \n",
    "models = {}\n",
    "n_gauss = 64 ### <---- LAB WORK: You can play with the amount of Gaussians and register the impact on performance\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    models[lang] = GaussianMixture(n_components=n_gauss, covariance_type='diag', max_iter=20, n_init=1, init_params='kmeans', verbose=2, verbose_interval=1)\n",
    "    \n",
    "for lang in LANGUAGES:\n",
    "    print(f'Training model for {lang}')\n",
    "    models[lang].fit(train_data[train_labels==lang])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models have been trained, we can store them in disk for later usage. Again, be careful and avoid storing versions of useless models. By default, the model is stored in a folder inside the data partition folder and contains the feature extraction in the name and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models     \n",
    "now = str(datetime.datetime.now()).replace(' ','_').split('.')[0]\n",
    "path = Path(DATADIR) / trainset / 'models'\n",
    "\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "model_name = f'gmm_{transform_id}_{now}'\n",
    "os.mkdir(f'{path}/{model_name}/')\n",
    "\n",
    "filename = f'{path}/{model_name}/model.pkl'\n",
    "pickle.dump(models, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the `sklearn` documentation and inspect the models trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['GER'].means_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you later need to reload your models (because you want to use them to predict on new data), you will have to do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELOAD MODEL!?!?\n",
    "# trainset = 'train100'\n",
    "# transform_id = 'mfcc_sdc_vad_cmvn'\n",
    "# time_label = '2024-04-13_01:40:14'\n",
    "\n",
    "trainset = 'train100'\n",
    "transform_id = 'mfcc13'\n",
    "time_label = '2024-04-12_18:18:35' ## <--- CHANGE THIS ACCORSINGLY\n",
    "\n",
    "filename = f'{DATADIR}/{trainset}/models/gmm_{transform_id}_{time_label}/model.pkl'\n",
    "models = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Classification of the dev set\n",
    "\n",
    "Now that we  already have trained models, let's predict/identify the native language in new audio data and test our model!!! \n",
    "\n",
    "But first, we need to obtain the development partition and apply the same feature extraction  as previously (using the ETS class).\n",
    "\n",
    "**IMPORTANT WARNING** Make sure to use the exact same feature extraction process as the one used for the train set. Otherwise, your model will be in disagreement with your evaluation data, and very likely, will not work at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "transform_id = 'mfcc13' ## <--- CHANGE THIS?\n",
    "\n",
    "dev_ets = ETS(DATADIR,'dev', \n",
    "                 transform_id=transform_id, \n",
    "                 audio_transform=transform[transform_id]['audio_transform'], \n",
    "                 chunk_transform=transform[transform_id]['chunk_transform'],\n",
    "                 chunk_size=transform[transform_id]['chunk_size'], \n",
    "                 chunk_hop=transform[transform_id]['chunk_hop']\n",
    "                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can iterate the data and use the models for scoring. We will store data in a dictionary, with keys corresponding to each file of the development and with value a dictionary containing the features and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dev_data = {}\n",
    "\n",
    "for data, label, basename in dev_ets:\n",
    "        if basename not in dev_data:\n",
    "                dev_data[basename] = {'data':[], 'label':label}\n",
    "        dev_data[basename]['data'].append(data)\n",
    "\n",
    "## We concatenate all the frames belonging to the same filename\n",
    "for basename in dev_data:\n",
    "        dev_data[basename]['data'] = np.concatenate(dev_data[basename]['data'])\n",
    "\n",
    "print(f'Finished reading all data in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the log-likelihood for every file and language model, obtain the prediction and store everything (including the filenames and the reference label) in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "results_dev = {}\n",
    "results_dev['ref'] =  np.empty(len(dev_data),dtype=np.int32)\n",
    "results_dev['hyp'] =  np.empty(len(dev_data),dtype=np.int32)\n",
    "results_dev['llhs'] = np.empty((len(dev_data), len(LANGUAGES)), dtype=np.float64)\n",
    "results_dev['fileids'] = list()\n",
    "\n",
    "for i, fileid in tqdm(enumerate(sorted(dev_data)), total=len(dev_data)):\n",
    "    data = dev_data[fileid]['data']  # the features\n",
    "    \n",
    "    results_dev['fileids'].append(fileid)     #fileid\n",
    "\n",
    "    # obtain the log-likelihood score for each model and store\n",
    "    results_dev['llhs'][i,:] = np.array([models[lang].score(data) for lang in LANGUAGES])\n",
    "\n",
    "    # store the reference. Notice that we only have this for the dev set, not for the eval\n",
    "    results_dev['ref'][i] = (LANG2ID[dev_data[fileid]['label']]) #reference\n",
    "\n",
    "    # Obtain the maximum likelihood nativelanguge estimation\n",
    "    ix = np.argmax(results_dev['llhs'][i,:])\n",
    "    results_dev['hyp'][i] = LANG2ID[LANGUAGES[ix]]\n",
    "    \n",
    "    \n",
    "print(f'Finished predicting all data in {time.time() - start}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the results object of the development set. We will use it later to generate the final submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "filename = f'{DATADIR}/{trainset}/models/{model_name}/dev.pkl'\n",
    "pickle.dump(results_dev, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.5 System Evaluation\n",
    "After running the previous cells, we obtained two arrays with the reference and hypothesis labels (we can also reload them in case we need them). We can use these to compute different evaluation metrics and inspect the performance (and potential problems) of our system. Of course, you will only be able to do this assessment with the development set, since you don't have access to the eval labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can for instance obtain a classification report summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref, hyp = results_dev['ref'], results_dev['hyp']\n",
    "print(classification_report(ref, hyp, target_names=LANGUAGES))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall accuracy (this will be the **main metric for system ranking**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ref, hyp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a confusion matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(ref, hyp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(ref, hyp)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.0, 3.0))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=12)\n",
    "plt.ylabel('Actuals', fontsize=12)\n",
    "plt.title(f'Confusion Matrix\\n(Accuracy {100*accuracy_score(ref, hyp):.2f})', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a form of approximate reference, these are the accuracies that can be obtained with the following configurations using the `train100` data set:\n",
    "\n",
    "|       |     Accuracy       |\n",
    "|-------|--------------------|\n",
    "| mfc13 | 0.5227272727272727 |\n",
    "| mfcc39_d_dd_vad_cmvn      |  0.6761363636363636                  |\n",
    "|  mfcc_sdc_vad_cmvn     |     0.7386363636363636               |   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are quite poor. The baseline system is very limited in several aspects (features, time context, generative model, etc.). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Classification of the evl partition\n",
    "\n",
    "Once you are happy with your system and the results obtained in the development set, you are ready to generate the predictions on the `'evl'` partition. To do that, you have to follow the same process as for the development partition, but of course, this time you will not be able to obtain performance results because you don't have labels for this partition. \n",
    "\n",
    "We start by instantiating the `ETS` class for the `'evl'` partition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "# RELOAD MODEL!?!?\n",
    "# trainset = 'train100'\n",
    "# transform_id = 'mfcc_sdc_vad_cmvn'\n",
    "# time_label = '2024-04-13_01:40:14'\n",
    "\n",
    "trainset = 'train100'\n",
    "transform_id = 'mfcc13'\n",
    "time_label = '2024-04-12_18:18:35' ## <--- CHANGE THIS ACCORSINGLT\n",
    "\n",
    "filename = f'{DATADIR}/{trainset}/models/gmm_{transform_id}_{time_label}/model.pkl'\n",
    "models = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "evl_ets = ETS(DATADIR,'evl', \n",
    "                 transform_id=transform_id, \n",
    "                 audio_transform=transform[transform_id]['audio_transform'], \n",
    "                 chunk_transform=transform[transform_id]['chunk_transform'],\n",
    "                 chunk_size=transform[transform_id]['chunk_size'], \n",
    "                 chunk_hop=transform[transform_id]['chunk_hop']\n",
    "                )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "evl_data = {}\n",
    "\n",
    "for data, label, basename in evl_ets:\n",
    "        if basename not in evl_data:\n",
    "                evl_data[basename] = {'data':[], 'label':label}\n",
    "        evl_data[basename]['data'].append(data)\n",
    "\n",
    "for basename in evl_data:\n",
    "        evl_data[basename]['data'] = np.concatenate(evl_data[basename]['data'])\n",
    "\n",
    "print(f'Finished reading all data in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, apply the model(s) to the new `'evl'` data and safe the results for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "results_evl = {}\n",
    "results_evl['ref'] =  None\n",
    "results_evl['hyp'] =  np.empty(len(evl_data),dtype=np.int32)\n",
    "results_evl['llhs'] = np.empty((len(evl_data), len(LANGUAGES)), dtype=np.float64)\n",
    "results_evl['fileids'] = list()\n",
    "\n",
    "\n",
    "# Obtain LLH matrix\n",
    "for i, fileid in tqdm(enumerate(sorted(evl_data)), total=len(evl_data)):\n",
    "\n",
    "    data = evl_data[fileid]['data']  # the features\n",
    "    results_evl['fileids'].append(fileid)     #fileid\n",
    "\n",
    "    # obtain the log-likelihood score for each model and store\n",
    "    results_evl['llhs'][i,:] = np.array([models[lang].score(data) for lang in LANGUAGES])\n",
    "\n",
    "    # Obtain the maximum likelihood languge estimation\n",
    "    ix = np.argmax(results_evl['llhs'][i,:])\n",
    "    results_evl['hyp'][i] = LANG2ID[LANGUAGES[ix]]    \n",
    "\n",
    "print(f'Finished predicting all data in {time.time() - start}')\n",
    "\n",
    "\n",
    "# save results\n",
    "filename = f'{DATADIR}/{trainset}/models/{model_name}/evl.pkl'\n",
    "pickle.dump(results_evl, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Submitting your system predictions to the challenge\n",
    "\n",
    "First, you need to create the predictions file in the expected format.\n",
    "The predictions file used for submission and scoring is a CSV file containing the predictions of both the `dev` and `evl` partitions.\n",
    "The file has two fields: fileId and Lang. The fileId is the unique audio file identifier and the Lang field is the language prediction (numeric from 1 to 4). The predictions file name must be as follows:\n",
    "\n",
    "`G<YY>_<SYSTEMID>.csv` \n",
    "\n",
    "where `<YY>` is the students' group number (use 2 digits) and `<SYSTEMID>` is an identifying string for that submission/system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = 'train100' ## <--- CHANGE THIS ACCORDINGLY\n",
    "transform_id = 'mfcc13' ## <--- CHANGE THIS ACCORDINGLY\n",
    "time_label = '2024-04-12_18:18:35' ## <--- CHANGE THIS ACCORDINGLY\n",
    "\n",
    "filename_dev = f'{DATADIR}/{trainset}/models/gmm_{transform_id}_{time_label}//dev.pkl'\n",
    "filename_evl = f'{DATADIR}/{trainset}/models/gmm_{transform_id}_{time_label}//evl.pkl'\n",
    "\n",
    "results_dev = pickle.load(open(filename_dev, 'rb'))\n",
    "results_evl = pickle.load(open(filename_evl, 'rb'))\n",
    "\n",
    "group, system = '00', f'baseline_train100_{transform_id}'\n",
    "with open(f'{CWD}/g{group}_{system}.csv', 'w') as file:\n",
    "    csv_writer = csv.writer(file) # CSV writer\n",
    "    csv_writer.writerow(('fileId', 'Lang')) # Header of the CSV\n",
    "\n",
    "    # Save dev results\n",
    "    for i in range(len(results_dev['fileids'])):\n",
    "        csv_writer.writerow((results_dev['fileids'][i], results_dev['hyp'][i]))\n",
    "    # Save evl results\n",
    "    for i in range(len(results_evl['fileids'])):\n",
    "        csv_writer.writerow((results_evl['fileids'][i], results_evl['hyp'][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can submit your prediction(s) in the following [Kaggle competition](https://www.kaggle.com/t/312cd4200cfb4e138ea9372ce5bc33fd).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contacts and support\n",
    "You can contact the professors during the classes or the office hours.\n",
    "\n",
    "Particularly, for this second laboratory assignment, you should contact Prof. Alberto Abad: alberto.abad@tecnico.ulisboa.pt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
